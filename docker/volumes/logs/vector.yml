api:
  enabled: true
  address: 0.0.0.0:9001

sources:
  docker_host:
    type: docker_logs
    exclude_containers:
      - supabase-vector

transforms:
  # Remove unnecessary fields and add common metadata
  project_logs:
    type: remap
    inputs:
      - docker_host
    source: |-
      # Set static project and reassign fields
      .project = "default"
      .event_message = del(.message)
      .appname = del(.container_name)
      # Common fields deletion in one block
      for field in ["container_created_at", "container_id", "source_type", "stream", "label", "image", "host"] {
        del(.[field])
      }

  # Route logs based on appname
  router:
    type: route
    inputs:
      - project_logs
    route:
      kong: '.appname == "supabase-kong"'
      auth: '.appname == "supabase-auth"'
      rest: '.appname == "supabase-rest"'
      realtime: '.appname == "supabase-realtime"'
      storage: '.appname == "supabase-storage"'
      functions: '.appname == "supabase-functions"'
      db: '.appname == "supabase-db"'

  ##############################################################################
  # Transformations per application log type
  ##############################################################################

  # For nginx logs from Kong, extract standard fields
  kong_logs:
    type: remap
    inputs:
      - router.kong
    source: |-
      req, err = parse_nginx_log(.event_message, "combined")
      if err == null {
          .timestamp = req.timestamp
          .metadata.request.headers.referer = req.referer
          .metadata.request.headers.user_agent = req.agent
          .metadata.request.headers.cf_connecting_ip = req.client
          .metadata.request.method = req.method
          .metadata.request.path = req.path
          .metadata.request.protocol = req.protocol
          .metadata.response.status_code = req.status
      } else {
          abort
      }

  # For Kong error logs, set defaults and parse error log details
  kong_err:
    type: remap
    inputs:
      - router.kong
    source: |-
      .metadata.request.method = "GET"
      .metadata.response.status_code = 200
      parsed, err = parse_nginx_log(.event_message, "error")
      if err == null {
          .timestamp = parsed.timestamp
          .severity = parsed.severity
          .metadata.request.host = parsed.host
          .metadata.request.headers.cf_connecting_ip = parsed.client
          url_parts, url_err = split(parsed.request, " ")
          if url_err == null and array_length(url_parts) >= 3 {
              .metadata.request.method = url_parts[0]
              .metadata.request.path = url_parts[1]
              .metadata.request.protocol = url_parts[2]
          }
      } else {
          abort
      }

  # Gotrue logs: Parse structured JSON and merge fields
  auth_logs:
    type: remap
    inputs:
      - router.auth
    source: |-
      parsed, err = parse_json(.event_message)
      if err == null {
          .metadata.timestamp = parsed.time
          .metadata = merge!(.metadata, parsed)
      }

  # PostgREST logs: Separate timestamp using regex extraction
  rest_logs:
    type: remap
    inputs:
      - router.rest
    source: |-
      parsed, err = parse_regex(.event_message, r'^(?P<time>.*): (?P<msg>.*)$')
      if err == null {
          .event_message = parsed.msg
          .timestamp = to_timestamp!(parsed.time)
          .metadata.host = .project
      }

  # Realtime logs: Parse logs to extract level and update metadata; use only relevant parts
  realtime_logs:
    type: remap
    inputs:
      - router.realtime
    source: |-
      # Pass project info as external id and remove redundant field
      .metadata.project = del(.project)
      .metadata.external_id = .metadata.project
      parsed, err = parse_regex(.event_message, r'^(?P<time>\d+:\d+:\d+\.\d+) \[(?P<level>\w+)\] (?P<msg>.*)$')
      if err == null {
          .event_message = parsed.msg
          .metadata.level = parsed.level
      }

  # Storage logs: Parse JSON if possible and attach common context info
  storage_logs:
    type: remap
    inputs:
      - router.storage
    source: |-
      .metadata.project = del(.project)
      .metadata.tenantId = .metadata.project
      parsed, err = parse_json(.event_message)
      if err == null {
          .event_message = parsed.msg
          .metadata.level = parsed.level
          .metadata.timestamp = parsed.time
          .metadata.context = [{
            host: parsed.hostname,
            pid: parsed.pid
          }]
      }

  # Postgres logs: Extract severity using regex and standardize output
  db_logs:
    type: remap
    inputs:
      - router.db
    source: |-
      .metadata.host = "db-default"
      .metadata.parsed.timestamp = .timestamp
      parsed, err = parse_regex(.event_message, r'.*(?P<level>INFO|NOTICE|WARNING|ERROR|LOG|FATAL|PANIC?):.*', numeric_groups: true)
      if err != null or parsed == null {
        .metadata.parsed.error_severity = "LOG"
      } else {
          .metadata.parsed.error_severity = upcase!(parsed.level)
          if .metadata.parsed.error_severity == "INFO" {
              .metadata.parsed.error_severity = "LOG"
          }
      }

sinks:
  logflare_auth:
    type: http
    inputs:
      - auth_logs
    encoding:
      codec: json
    method: post
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=gotrue.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'

  logflare_realtime:
    type: http
    inputs:
      - realtime_logs
    encoding:
      codec: json
    method: post
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=realtime.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'

  logflare_rest:
    type: http
    inputs:
      - rest_logs
    encoding:
      codec: json
    method: post
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=postgREST.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'

  logflare_db:
    type: http
    inputs:
      - db_logs
    encoding:
      codec: json
    method: post
    request:
      retry_max_duration_secs: 10
    # Routing through Kong to allow logflare initialisation before queries are processed
    uri: 'http://kong:8000/analytics/v1/api/logs?source_name=postgres.logs&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'

  logflare_functions:
    type: http
    inputs:
      - router.functions
    encoding:
      codec: json
    method: post
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=deno-relay-logs&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'

  logflare_storage:
    type: http
    inputs:
      - storage_logs
    encoding:
      codec: json
    method: post
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=storage.logs.prod.2&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'

  logflare_kong:
    type: http
    inputs:
      - kong_logs
      - kong_err
    encoding:
      codec: json
    method: post
    request:
      retry_max_duration_secs: 10
    uri: 'http://analytics:4000/api/logs?source_name=cloudflare.logs.prod&api_key=${LOGFLARE_API_KEY?LOGFLARE_API_KEY is required}'
