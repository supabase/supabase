---
title: 'Realtime Data Sync to Analytics Buckets'
subtitle: 'Replicate your PostgreSQL data to analytics buckets in real-time.'
---

<Admonition type="caution" title="This feature is in alpha">

Expect rapid changes, limited features, and possible breaking updates. [Share feedback](https://github.com/orgs/supabase/discussions/40116) as we refine the experience and expand access.

</Admonition>

By combining Supabase's **ETL service** with **Analytics Buckets**, you can build an end-to-end data warehouse solution that automatically syncs changes from your Postgres database to Iceberg tables.

## How it works

The ETL pipeline captures changes (INSERT, UPDATE, DELETE) from your Postgres database in real-time and writes them to your analytics bucket. This allows you to maintain an always-up-to-date data warehouse without impacting your production workloads.

## Setup steps

### Step 1: Create an Analytics bucket

First, create a new analytics bucket to store your replicated data:

1. Navigate to **Storage** in the Supabase Dashboard.
2. Click **Create Bucket**.
3. Enter a name (e.g., `my-warehouse`).
4. Select **Analytics Bucket** as the type.
5. Click **Create**.

<Image
  alt="Creating a new analytics bucket"
  src="/docs/img/database/replication/etl-iceberg-new-bucket.png"
  zoomable
/>

### Step 2: Create a publication

A publication defines which tables and change types will be replicated. Create one using SQL in the Supabase SQL Editor:

```sql
-- Create publication for tables you want to replicate
CREATE PUBLICATION pub_warehouse
  FOR TABLE users, orders, products;
```

This publication will track all changes (INSERT, UPDATE, DELETE) for the specified tables. For advanced use cases, see the [Replication Configuration Guide](/docs/guides/database/replication/etl-replication-setup).

### Step 3: Create the ETL pipeline

Now set up the pipeline to sync data to your analytics bucket:

1. Navigate to **Database > Replication** in the Supabase Dashboard.
2. Click **Create Pipeline**.
3. Select the **Publication** you created in Step 2.
4. Select your **Analytics Bucket** as the destination.
5. Configure any additional settings as needed.
6. Click **Create and Start**.

<Image
  alt="ETL pipeline configuration"
  src="/docs/img/database/replication/etl-iceberg-details.png"
  zoomable
/>

## Monitoring your pipeline

Once started, you can monitor the pipeline status directly in the **Database > Replication** section:

- **Status** - Shows if the pipeline is running, paused, or encountered errors
- **Sync Progress** - View the number of records replicated
- **Logs** - Check detailed logs for troubleshooting

## Next steps

Once data is flowing to your analytics bucket, you can:

- [Query with SQL via Postgres](/docs/guides/storage/analytics/query-with-postgres)
- [Connect with PyIceberg](/docs/guides/storage/analytics/examples/pyiceberg)
- [Analyze with Apache Spark](/docs/guides/storage/analytics/examples/apache-spark)

For advanced topics, see the [ETL Replication Monitoring Guide](/docs/guides/database/replication/etl-replication-monitoring).
