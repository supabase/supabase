---
id: 'etl-bigquery'
title: 'ETL to BigQuery'
description: 'Replicate your Supabase database to Google BigQuery using ETL Replication.'
subtitle: 'Stream data changes to BigQuery in real-time.'
sidebar_label: 'BigQuery'
---

BigQuery is Google's fully managed data warehouse. ETL Replication allows you to automatically sync your Supabase database tables to BigQuery for analytics and reporting.

To set up ETL Replication, follow the [ETL Replication Setup guide](/docs/guides/database/replication/etl-replication-setup).

## Prerequisites

Before configuring BigQuery as a destination, you need:

1. **Google Cloud Platform (GCP) account**: [Sign up for GCP](https://cloud.google.com/gcp)
2. **BigQuery dataset**: A [BigQuery dataset](https://cloud.google.com/bigquery/docs/datasets-intro) in your GCP project
3. **GCP service account key**: A [service account](https://cloud.google.com/iam/docs/keys-create-delete) in JSON format with the **BigQuery Data Editor** role

## BigQuery-specific settings

When adding BigQuery as a destination, you'll need to provide the following BigQuery-specific settings:

<Image
  alt="BigQuery Configuration Settings"
  src="/docs/img/database/replication/etl-bigquery-details.png"
  zoomable
/>

- **Project ID**: Your BigQuery project identifier (found in the GCP Console)
- **Dataset ID**: The name of your BigQuery dataset (without the project ID)

  <Admonition type="note">

  In the GCP Console, the dataset is shown as `project-id.dataset-id`. Enter only the part after the dot. For example, if you see `my-project.my_dataset`, enter `my_dataset`.

  </Admonition>

- **Service Account Key**: Your GCP service account key in JSON format. The service account must have the following permissions:
  - `bigquery.datasets.get`
  - `bigquery.tables.create`
  - `bigquery.tables.get`
  - `bigquery.tables.getData`
  - `bigquery.tables.update`
  - `bigquery.tables.updateData`

## How it works

Once configured, ETL Replication to BigQuery:

1. Captures changes from your Postgres database (INSERT, UPDATE, DELETE operations)
2. Batches changes for optimal performance
3. Creates BigQuery tables automatically to match your Postgres schema
4. Streams data to BigQuery with CDC metadata

<Admonition type="note">

Due to ingestion latency in BigQuery's streaming API, there may be a delay (typically seconds to minutes) in data appearing. This is normal and expected for BigQuery's architecture.

</Admonition>

### BigQuery CDC format

BigQuery tables include additional columns for change tracking:

- `_change_type`: The type of change (`INSERT`, `UPDATE`, `DELETE`)
- `_commit_timestamp`: When the change was committed in Postgres
- `_stream_id`: Internal identifier for the replication stream

## Querying replicated data

Once replication is running, you can query your data in BigQuery:

```sql
-- Query the replicated table
SELECT * FROM `your-project.your_dataset.users`
WHERE created_at > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY);

-- View CDC changes
SELECT
  _change_type,
  _commit_timestamp,
  id,
  name,
  email
FROM `your-project.your_dataset.users`
ORDER BY _commit_timestamp DESC
LIMIT 100;
```

## Limitations

BigQuery-specific limitations:

- **Ingestion latency**: BigQuery's streaming API has inherent latency (typically seconds to minutes)

For general ETL Replication limitations that apply to all destinations, see the [ETL Replication Setup guide](/docs/guides/database/replication/etl-replication-setup#limitations).

## Next steps

- [Set up ETL Replication](/docs/guides/database/replication/etl-replication-setup)
- [Monitor your replication](/docs/guides/database/replication/etl-replication-monitoring)
- [View ETL Replication FAQ](/docs/guides/database/replication/etl-replication-faq)
