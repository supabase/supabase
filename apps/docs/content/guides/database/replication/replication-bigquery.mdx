---
id: 'replication-bigquery'
title: 'Replicate to BigQuery'
description: 'Replicate your Supabase database to Google BigQuery using replication.'
subtitle: 'Stream data changes to BigQuery in real-time.'
sidebar_label: 'BigQuery'
---

<Admonition type="caution" label="Private Alpha">

Replication is currently in private alpha. Access is limited and features may change.

</Admonition>

BigQuery is Google's fully managed data warehouse. Replication powered by [Supabase ETL](https://github.com/supabase/etl) allows you to automatically sync your Supabase database tables to BigQuery for analytics and reporting.

<Admonition type="tip">

This page covers BigQuery-specific configuration. For complete setup instructions including publications, general settings, and pipeline management, see the [Replication Setup guide](/docs/guides/database/replication/replication-setup).

</Admonition>

### Setup

Setting up BigQuery replication requires preparing your GCP resources, then configuring BigQuery as a destination.

#### Step 1: Prepare GCP resources

Before configuring BigQuery as a destination, set up the following in Google Cloud Platform:

1. **Google Cloud Platform (GCP) account**: [Sign up for GCP](https://cloud.google.com/gcp) if you don't have one
2. **BigQuery dataset**: Create a [BigQuery dataset](https://cloud.google.com/bigquery/docs/datasets-intro) in your GCP project
3. **GCP service account key**: Create a [service account](https://cloud.google.com/iam/docs/keys-create-delete) with the **BigQuery Data Editor** role and download the JSON key file

#### Step 2: Add BigQuery as a destination

After preparing your GCP resources, configure BigQuery as a destination:

1. Navigate to [Database](/dashboard/project/_/database/replication) â†’ **Replication** in your Supabase Dashboard
2. Click **Add destination**
3. Configure the destination:

   <Image
     alt="BigQuery Configuration Settings"
     src="/docs/img/database/replication/replication-bigquery-details.png"
     zoomable
   />

   - **Destination type**: Select **BigQuery**
   - **Project ID**: Your BigQuery project identifier (found in the GCP Console)
   - **Dataset ID**: The name of your BigQuery dataset (without the project ID)

     <Admonition type="note">

     In the GCP Console, the dataset is shown as `project-id.dataset-id`. Enter only the part after the dot. For example, if you see `my-project.my_dataset`, enter `my_dataset`.

     </Admonition>

   - **Service Account Key**: Your GCP service account key in JSON format. The service account must have the following permissions:
     - `bigquery.datasets.get`
     - `bigquery.tables.create`
     - `bigquery.tables.get`
     - `bigquery.tables.getData`
     - `bigquery.tables.update`
     - `bigquery.tables.updateData`

4. Complete the remaining configuration following the [Replication Setup guide](/docs/guides/database/replication/replication-setup)

### How it works

Once configured, replication to BigQuery:

1. Captures changes from your Postgres database (INSERT, UPDATE, DELETE operations)
2. Batches changes for optimal performance
3. Creates BigQuery tables automatically to match your Postgres schema
4. Streams data to BigQuery with CDC metadata

<Admonition type="note">

Due to ingestion latency in BigQuery's streaming API, there may be a delay (typically seconds to minutes) in data appearing. This is normal and expected for BigQuery's architecture.

</Admonition>

#### BigQuery CDC format

BigQuery tables include additional columns for change tracking:

- `_change_type`: The type of change (`INSERT`, `UPDATE`, `DELETE`)
- `_commit_timestamp`: When the change was committed in Postgres
- `_stream_id`: Internal identifier for the replication stream

### Querying replicated data

Once replication is running, you can query your data in BigQuery:

```sql
-- Query the replicated table
SELECT * FROM `your-project.your_dataset.users`
WHERE created_at > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY);

-- View CDC changes
SELECT
  _change_type,
  _commit_timestamp,
  id,
  name,
  email
FROM `your-project.your_dataset.users`
ORDER BY _commit_timestamp DESC
LIMIT 100;
```

### Limitations

BigQuery-specific limitations:

- **Ingestion latency**: BigQuery's streaming API has inherent latency (typically seconds to minutes)
- **Row size**: Limited to 10 MB per row due to BigQuery Storage Write API constraints

For general replication limitations that apply to all destinations, see the [Replication Setup guide](/docs/guides/database/replication/replication-setup#limitations).

### Next steps

- [Set up replication](/docs/guides/database/replication/replication-setup)
- [Monitor replication](/docs/guides/database/replication/replication-monitoring)
- [View replication FAQ](/docs/guides/database/replication/replication-faq)
