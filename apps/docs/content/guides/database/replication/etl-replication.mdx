---
id: 'etl-replication'
title: 'ETL Replication'
description: 'Automatically replicate your database to external destinations like BigQuery and Analytics Buckets.'
subtitle: 'Supabase managed replication to external destinations.'
sidebar_label: 'ETL Replication'
---

ETL Replication is Supabase's managed solution for automatically syncing your data to external destinations. This could be a data warehouse, or a specialized service such as customer management or business analytics software.

<Admonition type="caution" label="Private Alpha">

ETL Replication is currently in private alpha. Features and functionality may change. To join our early access program, send an email to product-ops@supabase.io.

</Admonition>

<Admonition type="note">

This guide covers **ETL Replication**, Supabase's automated replication product. For manual replication using external tools, see [Manual Replication Setup](/docs/guides/database/replication/manual-replication-setup). For deploying read-only databases across multiple regions, see [Read Replicas](/docs/guides/platform/read-replicas) instead.

</Admonition>

## Getting started

To set up ETL Replication:

1. **[Create a publication](/docs/guides/database/replication/etl-replication-setup#step-1-create-a-publication)** - Define which tables and changes to replicate
2. **[Choose a destination](#destinations)** - Select BigQuery or Iceberg based on your needs
3. **[Configure replication](/docs/guides/database/replication/etl-replication-setup)** - Set up your destination in the Dashboard
4. **[Monitor replication](/docs/guides/database/replication/etl-replication-monitoring)** - Track status and troubleshoot issues

For common questions, see the [ETL Replication FAQ](/docs/guides/database/replication/etl-replication-faq).

## Destinations

Choose a destination based on your analytics and integration needs:

### BigQuery

Google's fully managed data warehouse for large-scale analytics.

- **Best for**: Google Cloud users, SQL analytics, business intelligence
- **Format**: Managed BigQuery tables with CDC metadata
- **Features**: Streaming inserts, automatic table creation, CDC history

[Learn more about BigQuery replication →](/docs/guides/database/replication/etl-bigquery)

### Iceberg (Analytics Buckets)

Apache Iceberg tables in S3-compatible storage for flexible analytics.

- **Best for**: Open-format data lakes, multi-tool analytics, data science
- **Format**: Iceberg tables with Parquet files
- **Features**: Time travel, schema evolution, S3 API access

[Learn more about Iceberg replication →](/docs/guides/database/replication/etl-iceberg)

<Admonition type="note">

More destinations will be added in the future. If you need to replicate to other destinations, consider using [Manual Replication](/docs/guides/database/replication/manual-replication-setup).

</Admonition>

## Advanced Settings

When configuring a destination, you can adjust these advanced settings:

- **Max fill milliseconds**: Number of milliseconds after which a batch should be sent, even if it hasn't reached its maximum size (default: 5000ms)
- **Max staleness minutes** (BigQuery only): BigQuery's [`max_staleness`](https://cloud.google.com/bigquery/docs/change-data-capture#manage_table_staleness) option controls how fresh the data must be

## Limitations

ETL Replication has several limitations to be aware of:

- **TOAST values**: Large values stored as [TOASTed values](https://www.postgresql.org/docs/current/storage-toast.html) are limited to 10MB

  <Admonition type="note" label="Workaround for TOAST columns">

  This limitation is due to the underlying [replica identity](https://www.postgresql.org/docs/current/sql-altertable.html#SQL-ALTERTABLE-REPLICA-IDENTITY). To work around this, set the table's replica identity to full (all columns), which forces Postgres to send TOAST columns in the CDC stream.

  </Admonition>

- **Custom data types**: Not supported
- **Event filtering**: All event types (INSERT, UPDATE, DELETE) are required
- **Column lists and row filters**: Not supported
- **Partitioned tables**: Not supported
- **Generated columns**: Not supported
- **Schema changes**: No automatic support for schema alterations
- **Primary keys**: Tables must have primary keys

## Next steps

- [Set up ETL Replication](/docs/guides/database/replication/etl-replication-setup)
- [Configure BigQuery destination](/docs/guides/database/replication/etl-bigquery)
- [Configure Iceberg destination](/docs/guides/database/replication/etl-iceberg)
- [Monitor ETL Replication](/docs/guides/database/replication/etl-replication-monitoring)
- [Review ETL Replication FAQ](/docs/guides/database/replication/etl-replication-faq)
