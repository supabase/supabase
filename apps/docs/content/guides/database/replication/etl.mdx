---
title: ETL
description: Replicate your Supabase database to external destinations
---

ETL simplifies the process of syncing your data to an external destination. This could be another database, a data warehouse, or a specialized service such as customer management or business analytics software.

<Admonition type="caution">

`ETL` is currently in private alpha.

</Admonition>

Syncing requires a source and a destination. The source is your Supabase database, defined in the [**Publications**](/dashboard/project/_/database/publications) section of the dashboard.

The destination is a third party location, defined in the [**Replication**](/dashboard/project/_/database/replication) section of the dashboard. See the [supported destinations](#destinations).

## Sources

Choose what data to sync by creating a publication in your database. A publication is a set of changes, filtered by table and change type.

### Managing publications

1. Go to the [Database](/dashboard/project/_/database/tables) page in the Dashboard.
2. Click on **Publications** in the sidebar.
3. To work with ETL, toggle **all** database events.
4. Control which tables broadcast changes by selecting **Source** and toggling each table.

<video width="99%" muted playsInline controls={true}>
  <source
    src="https://xguihxuzqibwxjnimxev.supabase.co/storage/v1/object/public/videos/docs/api/api-realtime.mp4"
    type="video/mp4"
  />
</video>

## Destinations

Currently available destinations:

- [BigQuery](#bigquery)
- [Analytics Buckets](#analytics-buckets)

### BigQuery

#### Prerequisites

Before setting up replication to BigQuery, you need:

1. A [Google Cloud Platform (GCP)](https://cloud.google.com/gcp) account
2. A [BigQuery dataset](https://cloud.google.com/bigquery/docs/datasets-intro) in your GCP project
3. A [GCP service account key](https://cloud.google.com/iam/docs/keys-create-delete) in JSON format

#### Configuring BigQuery replication

1. Navigate to the [Database](/dashboard/project/_/database/tables) section in your Supabase Dashboard and select the **Replication** tab.
2. Click **Add destination** to open the New Destination panel.
3. Configure the replication settings:

   - **Name**: Enter a descriptive name for your replication destination
   - **Publication**: Select an existing publication or create a new one
   - **Type**: Select "BigQuery" from the destination types
   - **Project Id**: Enter the GCP project ID
   - **Project's Dataset Id**: Enter the ID of the BigQuery Data set

    <Admonition type="note">

   The BigQuery Data set ID shows the project ID and Data set ID as one string, separated by a ".". The Data set ID is the part after the ".".

    </Admonition>

   - **Service Account Key**: Enter your GCP service account key

4. Review the [Advanced Settings](#advanced-settings) to change the limits for the replication.

   - **Max fill milliseconds**: Change are sent in batches. Max fill milliseconds are number of milliseconds after which a batch should be sent, even if it hasn't reached its maximum size.
   - **Max staleness minutes**: See BigQuery's [`max_staleness`](https://cloud.google.com/bigquery/docs/change-data-capture#manage_table_staleness) option

5. Click **Create and start** to start the replication process.

<Admonition type="note">

Due to ingestion latency in BigQuery, there may be a delay in the data flow.

</Admonition>

### Analytics buckets

#### Configuring analytics buckets replication

1. Navigate to the [Database](/dashboard/project/_/database/tables) section in your Supabase Dashboard and select the **Replication** tab.
2. Click **Add destination** to open the New Destination panel.
3. Configure the replication settings:
   - **Name**: Enter a descriptive name for your replication destination
   - **Publication**: Select an existing publication or create a new one
   - **Type**: Select "Analytics Bucket" from the destination types
   - **Bucket**: Select an analytics bucket
   - **Namespace**: Select a namespace in the analytics bucket
   - **Catalog Token**: Will be automatically retrieved from your project's [API Secret key](/dashboard/project/_/settings/api-keys/new)
   - **S3 Access Key ID**: Select an existing key id if you have its secret access key, or create a new one by selecting "Create a new key".
   - **S3 Secret Access Key**: If you selected an existing key id in **S3 Access Key ID**, enter the corresponding S3 secret access key.
4. Review the [Advanced Settings](#advanced-settings) to change the limits for the replication.

   - **Max fill milliseconds**: Change are sent in batches. Max fill milliseconds are number of milliseconds after which a batch should be sent, even if it hasn't reached its maximum size.

5. Click **Create and start** to start the replication process.

## Monitoring replication

After setting up replication, data begins flowing within a few minutes. You can monitor the replication status and review any errors directly from the [Supabase Dashboard](/dashboard/project/_/database/replication) and clicking the **View status** button for a replication. In status view for the replication pipeline, you can also view logs by clicking the **View logs** button.

## Limitations

- Large values stored as [TOASTed values](https://www.postgresql.org/docs/current/storage-toast.html) are replicated, but limited to 10MB.

  <Admonition type="note" label="Set replica identity to full for tables with TOAST columns">

  This limitation is due to the underlying [replica identity](https://www.postgresql.org/docs/current/sql-altertable.html#SQL-ALTERTABLE-REPLICA-IDENTITY). In most cases the default replica identity works, but Postgres doesn't send TOAST column values if they haven't changed. This makes it hard for downstream systems to apply the update.

  To workaround this limitation, we set the table's replica identity to full (all columns), forcing Postgres to send the TOAST columns in the CDC stream and helping downstream systems apply the update event.

  </Admonition>

- Custom data types are not supported.
- No support for picking event types to replicate. ETL needs all insert, update and delete events.
- No column lists and row filter conditions support.
- No partitioned tables support.
- No generated columns support.
- No support for changing schema in the source.
- Tables should have a primary key set. Tables without a primary key will not be replicated. You can see in the sources list which tables don't have a primary key.
