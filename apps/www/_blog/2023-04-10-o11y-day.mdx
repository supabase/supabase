---
title: Supabase Logs are now self-hosted
tags:
  - launch-week
date: '2023-04-XX'
to_depth: 3
author: chasers,ziinc
---

We are releasing Supabase Logs for both self-hosted users and CLI development, and have integrated Logflare as the observability component the Supabase development stack.

## Logflare Introduction

Since [Logflare joined Supabase](https://supabase.com/blog/supabase-acquires-logflare) over a year ago itâ€™s been quietly handling over 1 billion log events every day. These events come from various areas in the Supabase infrastructure, such as the API gateway, Postgres databases, Storage, Edge Functions, Auth, and Realtime.

Logflare ingests these log events and stores them into BigQuery. To handle and scale up to traffic generated by Supabase as well as Logflare's existing customers, we leverage a multi-node architecture for high availability. The cluster count averages at 6 high-compute nodes, steadily handling every spike that each customer, Supabase or otherwise, throws at it.

To utilize and expose all ingested Supabase Platform data to customers, we then leverage Logflare Endpoints to integrate into the Supabase Studio, powering the log query UIs and most timeseries charts. These charts live across the studio, such as the project home page and the new API reports.

## Self-hosting Logflare

Logflare was previously available under a BSL license prior to joining Supabase. Following our open-source philosophy, weâ€™ve now changed the license to [Apache 2.0](https://github.com/Logflare/logflare/blob/staging/LICENSE.md), aligning it with the rest of our open-source products.

In the past few months weâ€™ve made Logflare more developer-friendly for local development and self-hosting. While youâ€™re building a project, you can view and query your logs from any Supabase service, just as you would in our cloud Platform.

> ðŸ“¢ Check out the [new self-hosting docs](http://localhost:3001/docs/reference/self-hosting-analytics/introduction) to get Logflare up and running as your analytics server.
>
> It currently supports a BigQuery backend, and we are actively working on supporting more.

## The Ingestion Pipeline

Logflare receives Supabase log events via multiple methods. Some services, such as Postgres, use Vector to clean and forward log events to our the Logflare ingest API. Other services such as Realtime and Storage utilise direct one of our [integrations](https://github.com/Logflare/logflare#integrations) to directly send the log events. These then get processed and streamed into the BigQuery backend.

## The Querying Pipeline

Now that we have ingested all these log events, the hard part is to not only search through these log events, but to aggregate and analyse them at scale. Crunching many terabytes of data on each analytics query is costly and expensive, and exposing the ingested data to Supabase customers in a naive manner would cause our querying costs to skyrocket.

To solve these issues, we built and refined Logflare Endpoints, the querying engine that powers many realtime Supabase Platform features, such as the logs views, Logs Explorer, project home usage charts, Functions usage charts and the new API report.

With Endpoints, you can create HTTP API endpoints from any SQL query, including parameterised queries. Endpoints is a bit like a PostgREST view but with some benefits:

- Query parameters
  - Query parameters of your Endpoint query URL are provided as query string parameters to the SQL query.
- Read-through caching
  - Results from the query are cached in memory for fast response times.
  - A read-through cache provides results if cached results do not exist.
- Active cache warming
  - Query results are proactively warmed at a configurable interval for a combination of fast response times and as-realtime-as-needed data.
- Query sandboxing
  - If an Endpoint query contains a CTE and the sandbox option is selected, the Endpoint will inject the query string of the `sql` query parameter into the Endpoint SQL replacing the default query (the part of the SQL query after the CTE).
  - Endpoints parse SQL to allow `select` queries only. No DML or DDL statements are permitted to run through Logflare Endpoints.

This feature set has let Supabase build any view weâ€™ve needed on top of billions of log events processed daily.

### Logflare Endpoint Example

This example showcases a GitHub webhook that sends all event in all Supabase GitHub organizations to Logflare. This gives Logflare structured events where the payload of the Github webhook gets turned into metadata for a log event:

```json
{
  "event_message": "supabase/supabase | JohannesBauer97 | created",
  "id": "0d48b71d-91c5-4356-82c7-fdb299b625d0",
  "metadata": {
    "sender": {
      "id": 15695124,
      "login": "JohannesBauer97",
      "node_id": "MDQ6VXNlcjE1Njk1MTI0",
      "site_admin": false,
      "type": "User",
      "url": "https://api.github.com/users/JohannesBauer97"
    },
    "starred_at": "2023-03-30T20:33:55Z",
		...
  },
  "timestamp": 1680208436849642
}
```

Weâ€™re interested in the top contributors org wide, and this can be extracted by creating an Endpoint with SQL (in BigQuery dialect):

```sql
select
  count(t.timestamp) as count,
  s.login as gh_user
from
  `github.supabase.webhooks` as t
  cross join unnest(metadata) as m
  cross join unnest(m.sender) as s
where
  timestamp::date > current_date() - @day::int
group by
  gh_user
order by
  count desc
limit
  25
```

With this view in place, we can use Endpoints to provide an API that we can hit from our application:

```bash
curl "https://logflare.app/endpoints/query/69425db0-1cfb-48b4-84c7-2a872b6f0a61" \
 -H 'Content-Type: application/json; charset=utf-8' \
 -G -d "day=30"
```

Which gives us the response with the top org wide contributors for the last 30 days!

```json
{
  "result": [
    {
      "count": 23404,
      "gh_user": "vercel[bot]"
    },
    {
      "count": 10005,
      "gh_user": "joshenlim"
    },
    {
      "count": 7026,
      "gh_user": "MildTomato"
    },
    {
      "count": 6405,
      "gh_user": "fsansalvadore"
    },
    {
      "count": 5195,
      "gh_user": "saltcod"
    },
    {
      "count": 3454,
      "gh_user": "alaister"
    },
    {
      "count": 2691,
      "gh_user": "kevcodez"
    },
    {
      "count": 2117,
      "gh_user": "gregnr"
    },
    {
      "count": 1769,
      "gh_user": "Ziinc"
    },
    {
      "count": 1749,
      "gh_user": "chasers"
    },
    {
      "count": 1430,
      "gh_user": "Isaiah-Hamilton"
    },
    ...
  ]
}
```

We can then configure this Endpoint to cache results for a set interval of 10 minutes after the first API request, and proactively update those cached results every 2 minutes. This would result in 5 separate queries spread out Ã¡o cross the 10 minute cache time. Even if we were to throw tens of thousands of API requests at the endpoint, we would only sustain the cost of 5 queries to update the cached data.

The initial request is fast because Logflare also performs setup (such as partitioning) on our BigQuery tables appropriately. Subsequent requests made are extremely fast as they get cached in-memory.

The best part is that all these knobs can be tweaked based on the use case. If we have a hard realtime requirement, we can completely disable caching, or drop the proactive caching to update on a per-second basis.

## The Self-hosted Challenge

In order to execute the license change, we had to get rid of all closed-source dependencies. Prior to our efforts, Logflare relied on the closed source [General SQL Parser](https://www.sqlparser.com/) under a business licenses. However, it would not be an acceptable compromise under the Apache License.

This resulted in an effort to switch Logflare over to an open source alternative, and we settled on the rust-based [sqlparser-rs](https://github.com/sqlparser-rs/sqlparser-rs) library, and also ended up helping to contribute a [few fixes](https://github.com/sqlparser-rs/sqlparser-rs/pulls?q=is%3Apr+is%3Amerged+author%3AZiinc) upstream for the BigQuery dialect.

Besides the parser, a big part of getting Logflare working with Supabase locally was transforming the multi-tenant architecture into something that was self-hosting friendly and could be easily configurable. We also moved towards environment variable based configuration instead of compile-time configurations, and also exposed the Endpoints configurations necessary for Supabase Logs.

## Whatâ€™s Next?

To further integrate Logflare into the Supabase platform, we are building out 2 main areas: Management API, Multiple Backends.

### Management API

The Management API allows users to programmatically interact with Logflare server and manage their Logflare account and resources. This feature will be available for both Logflare customers and self-hosted users.

You can check out the preview of our OpenAPI spec here: [https://logflare.app/swaggerui](https://logflare.app/swaggerui)

![Swagger UI](/images/blog/lw7-o11y/swagger.png)

Not only that, we intend to expose user account provisioning to select partners. Soon, youâ€™ll be able to apply to become a Logflare Partner and provision Logflare accounts with the Partner API. Perfect if you want to resell a log analytics service from within your platform.

Contact us at growth@supabase.com to get in early on that waitlist.

## Multiple Backends

We understand that many of our customers are concerned about relying on closed-source products. Although Logflare currently supports a BigQuery backend, we hope to add support for other analytics optimized databases like Clickhouse. Beyond that, we can also support pushing data to other web services, allowing Logflare to fit into any data pipeline.
This would benefit our Supabase CLI integration, such that once Postgres support is available, Logflare would be able to integrate seamlessly out-of-the-box without the BigQuery requirement.

## Wrapping Up

Logflare has given Supabase the flexibility to quickly deploy features powered by an underlying structured event stream. Materializing metrics from an event stream is a powerful framework for delivering real-time views on analytics streams.

Logflare is the hub of analytics streams for Supabase. We look forward to giving Supabase customers the same superpower.
